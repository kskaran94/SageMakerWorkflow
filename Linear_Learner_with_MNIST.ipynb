{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Linear Learner with MNIST\n",
    "## Making a Binary Prediction of Whether a Handwritten Digit is a 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This an example to use Amazon Sagemaker. SageMaker allows one to build a ML pipeline easily. Building, training and deploying of ML models is less cumbersome with SageMaker\n",
    "\n",
    "In this example, I will be using a Amazon's marketplace algorithm (Linear Learner).\n",
    "The purpose of the notebook is to explain the usage of sagemaker and not the modeling aspect.The data used in this is mnist and problem is framed as binary classification.\n",
    "\n",
    "\n",
    "Amazon SageMaker's Linear Learner algorithm extends upon typical linear models by training many models in parallel, in a computationally efficient manner. Each model has a different set of hyperparameters, and then the algorithm finds the set that optimizes a specific criteria. This can provide substantially more accurate models than typical linear algorithms at the same, or lower, cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re\n",
    "import pickle \n",
    "import gzip\n",
    "import numpy as np \n",
    "import urllib.request\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "import sagemaker\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import sagemaker.amazon.common as smac\n",
    "from sagemaker import get_execution_role\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook assumes that you have an AWS account and an IAM user setup and using the notebook instance of Amazon SageMaker. For further reference please refer to this notebook https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permissions and environment variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was created and tested on an ml.t2.medium notebook instance.\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "1. The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "2. The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'test-karan-02'\n",
    "prefix = 'sagemaker_demo'\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we read the dataset from an online URL into memory, for preprocessing prior to training. This processing could be done in situ by Amazon Athena, Apache Spark in Amazon EMR, Amazon Redshift, etc., assuming the dataset is present in the appropriate location. Then, the next step would be to transfer the data to S3 for use in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 873 ms, sys: 314 ms, total: 1.19 s\n",
      "Wall time: 2.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load the dataset\n",
    "urllib.request.urlretrieve(\"http://deeplearning.net/data/mnist/mnist.pkl.gz\", \"mnist.pkl.gz\")\n",
    "with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset is imported, it's typical as part of the machine learning process to inspect the data, understand the distributions, and determine what type(s) of preprocessing might be needed. As an example, let's go ahead and look at one of the digits that is part of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (2,10)\n",
    "def show_digit(img, caption='', subplot=None):\n",
    "    if subplot==None:\n",
    "        _,(subplot)=plt.subplots(1,1)\n",
    "    imgr=img.reshape((28,28))\n",
    "    subplot.axis('off')\n",
    "    subplot.imshow(imgr, cmap='gray')\n",
    "    plt.title(caption)\n",
    "\n",
    "show_digit(train_set[0][30], 'This is a {}'.format(train_set[1][30]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since algorithms have particular input and output requirements, converting the dataset is also part of the process that a data scientist goes through prior to initiating training. In this particular case, the Amazon SageMaker implementation of Linear Learner takes recordIO-wrapped protobuf, where the data we have today is a pickle-ized numpy array on disk.\n",
    "\n",
    "Most of the conversion effort is handled by the Amazon SageMaker Python SDK, imported as sagemaker below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = np.array([t.tolist() for t in train_set[0]]).astype('float32')\n",
    "labels = np.where(np.array([t.tolist() for t in train_set[1]]) == 0, 1, 0).astype('float32')\n",
    "\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, vectors, labels)\n",
    "buf.seek(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created our recordIO-wrapped protobuf, we'll need to upload it to S3, so that Amazon SageMaker training can use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded training data location: s3://test-karan-02/sagemaker_demo/recordio-pb-data\n"
     ]
    }
   ],
   "source": [
    "key = 'recordio-pb-data'\n",
    "boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, key)).upload_fileobj(buf)\n",
    "s3_train_data = 's3://{}/{}/{}'.format(bucket, prefix, key)\n",
    "print('uploaded training data location: {}'.format(s3_train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also setup an output S3 location for the model artifact that will be output as the result of training with the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training artifacts will be uploaded to: s3://test-karan-02/sagemaker_demo/output\n"
     ]
    }
   ],
   "source": [
    "output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "print('training artifacts will be uploaded to: {}'.format(output_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the data preprocessed and available in the correct format for training, the next step is to actually train the model using the data. \n",
    "\n",
    "Again, we'll use the Amazon SageMaker Python SDK to kick off training, and monitor status until it is completed. . Despite the dataset being small, provisioning hardware and loading the algorithm container take time upfront."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = get_image_uri(boto3.Session().region_name, 'linear-learner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll kick off the base estimator, making sure to pass in the necessary hyperparameters. Notice:\n",
    "\n",
    "1. feature_dim is set to 784, which is the number of pixels in each 28 x 28 image.\n",
    "2. predictor_type is set to 'binary_classifier' since we are trying to predict whether the image is or is not a 0.\n",
    "3. mini_batch_size is set to 200. This value can be tuned for relatively minor improvements in fit and speed, but selecting a reasonable value relative to the dataset is appropriate in most cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "linear = sagemaker.estimator.Estimator(container,\n",
    "                                       role, \n",
    "                                       train_instance_count=1, \n",
    "                                       train_instance_type='ml.c4.xlarge',\n",
    "                                       output_path=output_location,\n",
    "                                       sagemaker_session=sess)\n",
    "linear.set_hyperparameters(feature_dim=784,\n",
    "                           predictor_type='binary_classifier',\n",
    "                           mini_batch_size=200)\n",
    "\n",
    "linear.fit({'train': s3_train_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Sample Output \n",
    "\n",
    "1. 2020-02-26 21:07:45 Starting - Starting the training job...\n",
    "2. 2020-02-26 21:07:46 Starting - Launching requested ML instances......\n",
    "3. 2020-02-26 21:08:53 Starting - Preparing the instances for training......\n",
    "4. 2020-02-26 21:09:48 Downloading - Downloading input data...\n",
    "5. 2020-02-26 21:10:45 Training - Training image download completed. Training in progress..Docker entrypoint called with argument(s): train\n",
    "\n",
    "6. 2020-02-26 21:28:11 Uploading - Uploading generated training model\n",
    "7. 2020-02-26 21:28:11 Completed - Training job completed\n",
    "\n",
    "\n",
    "Training seconds: 1103\n",
    "Billable seconds: 1103"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up hosting for the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've trained our model, we can deploy it behind an Amazon SageMaker real-time hosted endpoint. This will allow out to make predictions (or inference) from the model dyanamically.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_predictor = linear.deploy(initial_instance_count=1,\n",
    "                                 instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Validate the model for use\n",
    "Finally, we can now validate the model for use. We can pass HTTP POST requests to the endpoint to get back predictions. To make this easier, we'll again use the Amazon SageMaker Python SDK and specify how to serialize requests and deserialize responses that are specific to the algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
